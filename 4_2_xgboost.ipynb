{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost Training**\n",
    "\n",
    "In this notebook, we will build upon the baseline model we previously trained using ElasticNet and aim to improve the predictive performance by utilizing the **XGBoost** algorithm. XGBoost is a highly efficient and popular machine learning algorithm that performs well on both structured and unstructured data. Its strength lies in its ability to handle complex, non-linear relationships and its robustness to overfitting when properly tuned.\n",
    "\n",
    "Also in this case, we will evaluate the model using two different datasets:\n",
    "1. **Base Dataset**: This dataset contains the original features.\n",
    "2. **Graph Dataset**: This dataset includes additional features derived from graph-based representations of the data.\n",
    "\n",
    "For model training and evaluation, we will utilize **TimeSeriesSplit** as our cross-validation technique. This choice ensures that the training and validation sets respect the temporal order of the data.\n",
    "\n",
    "To optimize the hyperparameters of the XGBoost model, we will use **Optuna**, a powerful hyperparameter optimization framework. Unlike traditional grid search, which exhaustively searches through a predefined set of hyperparameters, Optuna uses a more efficient approach by leveraging a dynamic optimization algorithm to explore the hyperparameter space. This approach is more resource-efficient and can lead to better model performance by finding optimal settings for parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_PATH = Path(\"data/processed\")\n",
    "RESULTS_PATH = Path(\"results\")\n",
    "MODELS_PATH = Path(\"models\")\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(dataset_name, df):\n",
    "    print(f\"Training on dataset: {dataset_name}\")\n",
    "    \n",
    "    target_col = \"stop_arrival_delay\"\n",
    "    df = df.sort_values(by=[\"month\", \"day_of_week\", \"hour\"])    # Important the order for time series cross-validation\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Define optimization function for XGBoost\n",
    "    def objective_xgb_gpu(trial):\n",
    "        xgb_params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 1),\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            \"seed\": 42,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"device\": \"cuda\",\n",
    "        }\n",
    "\n",
    "        rmse_scores = []\n",
    "        mae_scores = []\n",
    "        r2_scores = []\n",
    "        for train_idx, val_idx in tscv.split(X_scaled, y):\n",
    "            X_train, X_val = X_scaled.iloc[train_idx], X_scaled.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            model = xgb.XGBRegressor(**xgb_params, n_estimators=500, early_stopping_rounds=50)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            preds = model.predict(X_val)\n",
    "\n",
    "            rmse_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "            mae_scores.append(mean_absolute_error(y_val, preds))\n",
    "            r2_scores.append(r2_score(y_val, preds))\n",
    "\n",
    "        return np.mean(rmse_scores)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective_xgb_gpu, n_trials=50)\n",
    "    \n",
    "    print(f\"Best parameters for {dataset_name}: {study.best_params}\")\n",
    "    \n",
    "    # Train final model with best hyperparameters\n",
    "    best_model = xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        eval_metric=\"rmse\",\n",
    "        n_estimators=500,\n",
    "        early_stopping_rounds=50,\n",
    "        seed=42,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        **study.best_params,\n",
    "    )\n",
    "    best_model.fit(X_scaled, y, eval_set=[(X_scaled, y)], verbose=False)\n",
    "    \n",
    "    model_filename = MODELS_PATH / f\"xgb_{dataset_name.lower()}.pkl\"\n",
    "    results_filename = RESULTS_PATH / f\"xgb_results_{dataset_name.lower()}.csv\"\n",
    "    \n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    \n",
    "    results[dataset_name] = {\n",
    "        \"RMSE\": final_rmse,\n",
    "        \"MAE\": final_mae,\n",
    "        \"R²\": final_r2\n",
    "    }\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        \"RMSE\": [final_rmse],\n",
    "        \"MAE\": [final_mae],\n",
    "        \"R²\": [final_r2]\n",
    "    }).to_csv(results_filename, index=False)\n",
    "    \n",
    "    print(f\"Model saved: {model_filename}\")\n",
    "    print(f\"Results saved: {results_filename}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sorted_idx = np.argsort(best_model.feature_importances_)[::-1]\n",
    "    plt.bar(range(len(X.columns)), best_model.feature_importances_[sorted_idx], align=\"center\")\n",
    "    plt.xticks(range(len(X.columns)), [X.columns[i] for i in sorted_idx], rotation=90)\n",
    "    plt.title(f\"Feature Importance - {dataset_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset: Base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 05:05:32,874] A new study created in memory with name: no-name-35f58862-f2e3-49ab-85e2-d00eeead9fa5\n",
      "c:\\Users\\Ningo\\projects\\dynamic-train-delay-prediction-system\\.venv\\lib\\site-packages\\xgboost\\core.py:729: UserWarning: [05:05:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "[I 2025-03-31 05:05:50,591] Trial 0 finished with value: 5.970750625017291 and parameters: {'eta': 0.22190295216304592, 'max_depth': 7, 'subsample': 0.5782696787692632, 'colsample_bytree': 0.9009048302457557, 'gamma': 0.9544283962573178, 'lambda': 2.1965363999838487e-06, 'alpha': 0.31042486975358663}. Best is trial 0 with value: 5.970750625017291.\n",
      "[I 2025-03-31 05:06:22,762] Trial 1 finished with value: 5.933997965357614 and parameters: {'eta': 0.0797781767644686, 'max_depth': 6, 'subsample': 0.8818952280561123, 'colsample_bytree': 0.7110250601798981, 'gamma': 0.1194532649649458, 'lambda': 2.9776627268233226e-05, 'alpha': 0.0001201679867068963}. Best is trial 1 with value: 5.933997965357614.\n",
      "[I 2025-03-31 05:06:39,076] Trial 2 finished with value: 5.955547183759407 and parameters: {'eta': 0.2606118087575526, 'max_depth': 6, 'subsample': 0.5405131367328085, 'colsample_bytree': 0.9957225978778941, 'gamma': 0.34568095886562544, 'lambda': 2.518576576205551e-08, 'alpha': 4.842100961793627e-06}. Best is trial 1 with value: 5.933997965357614.\n",
      "[I 2025-03-31 05:07:03,645] Trial 3 finished with value: 5.9716732874215275 and parameters: {'eta': 0.29571435326042755, 'max_depth': 3, 'subsample': 0.5885078205490408, 'colsample_bytree': 0.8899453183131039, 'gamma': 0.7325813276740395, 'lambda': 0.005735976472931852, 'alpha': 2.7640135236284127e-08}. Best is trial 1 with value: 5.933997965357614.\n",
      "[I 2025-03-31 05:07:26,129] Trial 4 finished with value: 5.970974329667544 and parameters: {'eta': 0.17275398165304987, 'max_depth': 10, 'subsample': 0.5541729776060484, 'colsample_bytree': 0.7511524025871307, 'gamma': 0.6949002474810481, 'lambda': 0.0003422658166533847, 'alpha': 0.021869929262012013}. Best is trial 1 with value: 5.933997965357614.\n",
      "[I 2025-03-31 05:08:28,977] Trial 5 finished with value: 5.927826678425076 and parameters: {'eta': 0.02214208777406956, 'max_depth': 9, 'subsample': 0.8532926829774343, 'colsample_bytree': 0.5773627247690879, 'gamma': 0.6621342550312077, 'lambda': 0.022718038689141594, 'alpha': 0.0006193830436279663}. Best is trial 5 with value: 5.927826678425076.\n",
      "[I 2025-03-31 05:09:01,053] Trial 6 finished with value: 5.933620118863932 and parameters: {'eta': 0.09438488778981784, 'max_depth': 6, 'subsample': 0.7863047505159155, 'colsample_bytree': 0.6827786880370641, 'gamma': 0.525946532114237, 'lambda': 8.352395672057683e-08, 'alpha': 5.0430275064443766e-05}. Best is trial 5 with value: 5.927826678425076.\n",
      "[I 2025-03-31 05:09:25,208] Trial 7 finished with value: 5.958636646972276 and parameters: {'eta': 0.2726411405351249, 'max_depth': 4, 'subsample': 0.689159793731964, 'colsample_bytree': 0.5818388067767661, 'gamma': 0.33803091952383413, 'lambda': 0.21303402933747997, 'alpha': 0.005886086158020398}. Best is trial 5 with value: 5.927826678425076.\n",
      "[I 2025-03-31 05:09:41,135] Trial 8 finished with value: 5.955508079493429 and parameters: {'eta': 0.27264243277373085, 'max_depth': 6, 'subsample': 0.6249553913947623, 'colsample_bytree': 0.7350417710388162, 'gamma': 0.571534876130229, 'lambda': 3.778535996762177e-05, 'alpha': 0.0008392321662038178}. Best is trial 5 with value: 5.927826678425076.\n",
      "[I 2025-03-31 05:10:04,489] Trial 9 finished with value: 5.9703137486157605 and parameters: {'eta': 0.2514183264857555, 'max_depth': 3, 'subsample': 0.7565599882150029, 'colsample_bytree': 0.9696259562671958, 'gamma': 0.4066059227601897, 'lambda': 0.09401682025347499, 'alpha': 0.020193151196066402}. Best is trial 5 with value: 5.927826678425076.\n",
      "[I 2025-03-31 05:11:23,829] Trial 10 finished with value: 5.924967875669212 and parameters: {'eta': 0.026621938859844565, 'max_depth': 10, 'subsample': 0.9956187972223589, 'colsample_bytree': 0.5000744931298923, 'gamma': 0.9865887430516835, 'lambda': 0.00436465327995672, 'alpha': 2.0443685241039563e-07}. Best is trial 10 with value: 5.924967875669212.\n",
      "[I 2025-03-31 05:12:45,616] Trial 11 finished with value: 5.934122068744576 and parameters: {'eta': 0.018575162741266266, 'max_depth': 10, 'subsample': 0.9933850365881902, 'colsample_bytree': 0.5276614500046267, 'gamma': 0.9877075007312305, 'lambda': 0.004645931630058208, 'alpha': 2.2825418998050964e-07}. Best is trial 10 with value: 5.924967875669212.\n",
      "[I 2025-03-31 05:13:37,941] Trial 12 finished with value: 5.939866346094716 and parameters: {'eta': 0.02457476133657217, 'max_depth': 8, 'subsample': 0.9901602210349953, 'colsample_bytree': 0.5066114467747125, 'gamma': 0.8086021999029912, 'lambda': 0.006898483097525659, 'alpha': 2.0545382224920736e-06}. Best is trial 10 with value: 5.924967875669212.\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"Base\": pd.read_parquet(PROCESSED_PATH / \"final_data.parquet\"),\n",
    "    \"Graph\": pd.read_parquet(PROCESSED_PATH / \"final_data_graph.parquet\"),\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    train_xgboost(name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename = RESULTS_PATH / \"xgb_results_base.csv\"\n",
    "results_df = pd.read_csv(results_filename)\n",
    "\n",
    "print(\"Results from xgb_results_base:\")\n",
    "print(results_df)\n",
    "\n",
    "results_filename = RESULTS_PATH / \"xgb_results_graph.csv\"\n",
    "results_df = pd.read_csv(results_filename)\n",
    "\n",
    "print(\"Results from xgb_results_graph:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PROCESSED_PATH / \"final_data.parquet\")\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**  \n",
    "\n",
    "Contrary to our expectations, the extra graph-based features may not add useful predictive power. In fact, they could be introducing noise rather than improving the model. We will see if this behavior is confirmed by later models.\n",
    "\n",
    "But leaving out the comparison for a moment, let's analyze the result in general. \n",
    "\n",
    "As df.describe() shows at the beginning of this notebook, the target variable (`stop_arrival_delay`) has the following characteristics:  \n",
    "- **Mean:** 2.95  \n",
    "- **Standard deviation:** 7.73  \n",
    "- **Median (50th percentile):** 1.0  \n",
    "- **75th percentile:** 4.0  \n",
    "- **Max:** 300.0\n",
    "\n",
    "This means that:\n",
    "- The median delay is just 1 minute, meaning most delays are small (ss already seen in data exploration).  \n",
    "- The mean (2.95) is higher than the median, suggesting right-skewed distribution (some extreme delays).  \n",
    "- The high standard deviation (7.73) indicates wide variability, likely due to rare but extreme delays.  \n",
    "\n",
    "In other words, most train delays are characterized by relatively small time variations. The majority of delays are concentrated in the lower range, with occasional extreme outliers. This distribution makes RMSE an ideal metric for our model evaluation for several key reasons. In fact, transportation systems prioritize consistency and minimal deviation. By optimizing for RMSE, we minimize frequent, small prediction errors, we reduce the overall variability in delay predictions and we create a model that performs consistently across most scenarios\n",
    "\n",
    "In our specific case, RMSE of ~2.07 means that, on average, the model's prediction error is around 2.07 minutes. Since most delays are small (0–4 minutes), an RMSE of 2.07 minutes suggests the model performs **reasonably well** in predicting common short delays. Of course, this metric alone does not tell us how the model behaves when predicting **large delays**, that we can consider outliers. \n",
    "The model may underpredict large delays, producing estimates that are closer to the mean (2.95 min) rather than accurately capturing outliers.  \n",
    "\n",
    "Let us continue with our evaluation, and look at the values of additional metrics, this time on the entire datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(dataset_name, df):\n",
    "\n",
    "#     model_filename = MODELS_PATH / f\"xgb_{dataset_name.lower()}.pkl\"\n",
    "#     with open(model_filename, \"rb\") as f:\n",
    "#         model = pickle.load(f)\n",
    "    \n",
    "#     target_col = \"stop_arrival_delay\"\n",
    "#     df = df.sort_values(by=[\"month\", \"day_of_week\", \"hour\"])\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y = df[target_col]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "    \n",
    "#     y_pred = model.predict(X_scaled)\n",
    "    \n",
    "#     mae = mean_absolute_error(y, y_pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "#     r2 = r2_score(y, y_pred)\n",
    "    \n",
    "#     print(f\"{dataset_name} Evaluation Results:\")\n",
    "#     print(f\"MAE: {mae:.4f}\")\n",
    "#     print(f\"RMSE: {rmse:.4f}\")\n",
    "#     print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "#     return {\"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
    "\n",
    "\n",
    "# results_base = evaluate_model(\"Base\", df_base, target_col)\n",
    "\n",
    "# results_graph = evaluate_model(\"Graph\", df_graph, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model_filename, dataset_name, X, y):\n",
    "#     print(f\"\\nEvaluating model: {dataset_name}\")\n",
    "    \n",
    "#     with open(model_filename, \"rb\") as f:\n",
    "#         model = pickle.load(f)\n",
    "\n",
    "#     y_pred = model.predict(X)\n",
    "\n",
    "#     mae = mean_absolute_error(y, y_pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "#     r2 = r2_score(y, y_pred)\n",
    "\n",
    "#     print(f\"{dataset_name} - MAE: {mae:.4f}\")\n",
    "#     print(f\"{dataset_name} - RMSE: {rmse:.4f}\")\n",
    "#     print(f\"{dataset_name} - R^2 Score: {r2:.4f}\")\n",
    "\n",
    "#     return {\"Dataset\": dataset_name, \"MAE\": mae, \"RMSE\": rmse, \"R^2\": r2}\n",
    "\n",
    "# datasets = {\n",
    "#     \"Base\": pd.read_parquet(PROCESSED_PATH / \"final_data.parquet\"),\n",
    "#     \"Graph\": pd.read_parquet(PROCESSED_PATH / \"final_data_graph.parquet\"),\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "# for name, df in datasets.items():\n",
    "#     df = df.sort_values(by=[\"month\", \"day_of_week\", \"hour\"])\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y = df[target_col]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "#     model_filename = MODELS_PATH / f\"xgb_{name.lower()}.pkl\"\n",
    "#     res = evaluate_model(model_filename, name, X_scaled, y)\n",
    "#     results.append(res)\n",
    "\n",
    "# df_results = pd.DataFrame(results)\n",
    "# df_results.to_csv(RESULTS_PATH / \"xgb_evaluation_metrics.csv\", index=False)\n",
    "\n",
    "# print(\"\\Metrics saved in results/evaluation_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train XGB on dataset with Embeddings**\n",
    "\n",
    "In the initial phase of experimentation, the XGBoost models were trained and evaluated on two datasets: the base dataset and an extended version enriched with handcrafted graph-derived features. Following these experiments, additional LSTM models were developed and tested, including a variant trained on **graph embeddings** generated through a Node2Vec algorithm applied to the railway network graph.\n",
    "\n",
    "Interestingly, the results showed that the LSTM model trained on graph embeddings outperformed the other LSTM variants, indicating that the embeddings were able to capture relevant structural information from the railway network, beyond what was explicitly encoded in the handcrafted graph features.\n",
    "\n",
    "Motivated by these findings, we decided to further investigate the contribution of graph embeddings by integrating them into the XGBoost framework. This decision is based on the hypothesis that, given XGBoost's superior performance and lower variance compared to LSTM models, combining it with the additional structural knowledge embedded in the graph embeddings may lead to further improvements in predictive accuracy without compromising efficiency and interpretability.\n",
    "\n",
    "This experiment aims to assess whether the latent structural relationships learned through the Node2Vec embeddings can enhance XGBoost's predictive capabilities, providing a more informative feature space for the regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_PATH = Path(\"data/processed\")\n",
    "MODELS_PATH = Path(\"models\")\n",
    "RESULTS_PATH = Path(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PROCESSED_PATH / \"train_data_fe.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.read_csv(Path(\"other\") / \"station_embeddings.csv\")\n",
    "embedding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(embedding, left_on=\"stop_name\", right_on=\"station_name\", how=\"left\")\n",
    "df.drop(columns=[\"station_name\"], inplace=True)\n",
    "\n",
    "df = df.sort_values(by=[\"train_number\", \"month\", \"day_of_week\", \"hour\"])\n",
    "\n",
    "df[\"prev_stop_departure_delay\"] = df.groupby(\"train_number\")[\"stop_departure_delay\"].shift(1)\n",
    "\n",
    "df.loc[df.groupby(\"train_number\").head(1).index, \"prev_stop_departure_delay\"] = np.nan\n",
    "\n",
    "df = df.drop(columns=[\"stop_departure_delay\"])\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "drop_cols = [\n",
    "    \"scheduled_departure_time\", \"scheduled_arrival_time\", \"stop_departure_time\",\n",
    "    \"departure_station\", \"arrival_station\", \"stop_name\"\n",
    "]\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "df[\"train_avg_delay\"] = df.groupby(\"train_number\")[\"stop_arrival_delay\"].transform(\"mean\")\n",
    "df.drop(columns=[\"train_number\"], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df = df.drop(columns=[\"prev_stop_departure_delay\"])\n",
    "\n",
    "df.to_parquet(PROCESSED_PATH / \"final_data_embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embedding_cols = [col for col in df.columns if col.startswith('0') or col.isdigit()]\n",
    "pca = PCA(n_components=10)\n",
    "embedding_pca = pca.fit_transform(df[embedding_cols])\n",
    "\n",
    "pca_cols = [f\"embed_pca_{i}\" for i in range(embedding_pca.shape[1])]\n",
    "df[pca_cols] = embedding_pca\n",
    "df.drop(columns=embedding_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_splits = 5\n",
    "# target_col = \"stop_arrival_delay\"\n",
    "# tscv = TimeSeriesSplit(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Embeddings\"\n",
    "train_xgboost(dataset_name, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename = RESULTS_PATH / \"xgb_results_embeddings.csv\"\n",
    "results_df = pd.read_csv(results_filename)\n",
    "\n",
    "print(\"Results from xgb_results_embeddings:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model_filename, dataset_name, X, y):\n",
    "#     print(f\"\\nEvaluating model: {dataset_name}\")\n",
    "    \n",
    "#     with open(model_filename, \"rb\") as f:\n",
    "#         model = pickle.load(f)\n",
    "\n",
    "#     y_pred = model.predict(X)\n",
    "\n",
    "#     mae = mean_absolute_error(y, y_pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "#     r2 = r2_score(y, y_pred)\n",
    "\n",
    "#     print(f\"{dataset_name} - MAE: {mae:.4f}\")\n",
    "#     print(f\"{dataset_name} - RMSE: {rmse:.4f}\")\n",
    "#     print(f\"{dataset_name} - R² Score: {r2:.4f}\")\n",
    "\n",
    "#     return {\"Dataset\": dataset_name, \"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
    "\n",
    "# datasets = {\n",
    "#     \"Base\": pd.read_parquet(PROCESSED_PATH / \"final_data.parquet\"),\n",
    "#     \"Graph\": pd.read_parquet(PROCESSED_PATH / \"final_data_graph.parquet\"),\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "# for name, df in datasets.items():\n",
    "#     df = df.sort_values(by=[\"month\", \"day_of_week\", \"hour\"])\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y = df[target_col]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "#     model_filename = MODELS_PATH / f\"xgb_{name.lower()}.pkl\"\n",
    "#     res = evaluate_model(model_filename, name, X_scaled, y)\n",
    "#     results.append(res)\n",
    "\n",
    "# df_results = pd.DataFrame(results)\n",
    "# df_results.to_csv(RESULTS_PATH / \"xgb_evaluation_metrics.csv\", index=False)\n",
    "\n",
    "# print(\"\\Metrics saved in results/evaluation_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_xgboost(df):\n",
    "#     df = df.sort_values(by=[\"month\", \"day_of_week\", \"hour\"])\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y = df[target_col]\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "#     def objective_xgb_gpu(trial):\n",
    "#         xgb_params = {\n",
    "#             \"objective\": \"reg:squarederror\",\n",
    "#             \"eval_metric\": \"rmse\",\n",
    "#             \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3),\n",
    "#             \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "#             \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#             \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "#             \"gamma\": trial.suggest_float(\"gamma\", 0, 1),\n",
    "#             \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#             \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "#             \"seed\": 42,\n",
    "#             \"tree_method\": \"hist\",\n",
    "#             \"device\": \"cuda\",\n",
    "#         }\n",
    "\n",
    "#         rmse_scores = []\n",
    "#         for train_idx, val_idx in tscv.split(X_scaled, y):\n",
    "#             X_train, X_val = X_scaled.iloc[train_idx], X_scaled.iloc[val_idx]\n",
    "#             y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "#             model = xgb.XGBRegressor(**xgb_params, n_estimators=500, early_stopping_rounds=50)\n",
    "#             model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "#             preds = model.predict(X_val)\n",
    "#             rmse_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "\n",
    "#         return np.mean(rmse_scores)\n",
    "\n",
    "#     study = optuna.create_study(direction=\"minimize\")\n",
    "#     study.optimize(objective_xgb_gpu, n_trials=50)\n",
    "    \n",
    "#     print(f\"Best parameters: {study.best_params}\")\n",
    "    \n",
    "#     best_model = xgb.XGBRegressor(\n",
    "#         objective=\"reg:squarederror\",\n",
    "#         eval_metric=\"rmse\",\n",
    "#         n_estimators=500,\n",
    "#         early_stopping_rounds=50,\n",
    "#         seed=42,\n",
    "#         tree_method=\"hist\",\n",
    "#         device=\"cuda\",\n",
    "#         **study.best_params,\n",
    "#     )\n",
    "#     best_model.fit(X_scaled, y, eval_set=[(X_scaled, y)], verbose=False)\n",
    "    \n",
    "#     model_filename = MODELS_PATH / \"xgb_embedding.pkl\"\n",
    "#     results_filename = RESULTS_PATH / \"xgb_embedding_results.csv\"\n",
    "    \n",
    "#     with open(model_filename, \"wb\") as f:\n",
    "#         pickle.dump(best_model, f)\n",
    "    \n",
    "#     pd.DataFrame({\"RMSE\": [study.best_value]}).to_csv(results_filename, index=False)\n",
    "#     print(f\"Model saved: {model_filename}\")\n",
    "#     print(f\"RMSE saved: {results_filename}\")\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     sorted_idx = np.argsort(best_model.feature_importances_)[::-1]\n",
    "#     plt.bar(range(len(X.columns)), best_model.feature_importances_[sorted_idx], align=\"center\")\n",
    "#     plt.xticks(range(len(X.columns)), [X.columns[i] for i in sorted_idx], rotation=90)\n",
    "#     plt.title(\"Feature Importance\")\n",
    "#     plt.show()\n",
    "\n",
    "# def evaluate_model(model_filename, X, y):\n",
    "#     print(\"\\nEvaluating model\")\n",
    "#     with open(model_filename, \"rb\") as f:\n",
    "#         model = pickle.load(f)\n",
    "#     y_pred = model.predict(X)\n",
    "#     mae = mean_absolute_error(y, y_pred)\n",
    "#     rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "#     r2 = r2_score(y, y_pred)\n",
    "#     print(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}, R² Score: {r2:.4f}\")\n",
    "#     return {\"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xgboost(df)\n",
    "\n",
    "# X = df.drop(columns=[target_col])\n",
    "# y = df[target_col]\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "# results = evaluate_model(MODELS_PATH / \"xgb_embedding.pkl\", X_scaled, y)\n",
    "# pd.DataFrame([results]).to_csv(RESULTS_PATH / \"xgb_embedding_evaluation_metrics.csv\", index=False)\n",
    "# print(\"Metrics saved in results/xgb_embedding_evaluation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind integrating graph embeddings into the XGBoost model has proven to be correct. The model trained on the dataset enriched with Node2Vec embeddings achieved the best performance among all XGBoost variants tested, confirming that the structural information encoded in the embeddings effectively enhances the model's ability to predict train delays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
